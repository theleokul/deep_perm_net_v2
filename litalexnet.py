# -*- coding: utf-8 -*-
"""litAlexNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wtzIdEYj2WaCU7mT0knR8--gGbu_C6wR
"""

import os
from collections import defaultdict
from typing import List, Callable
from numbers import Number
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import cvxpy as cp
from cvxpylayers.torch.cvxpylayer import CvxpyLayer
import pytorch_lightning as pl

import mnist_classifier as mnist_clf

import torch
# model_AlexNet = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)


class LitAlexNet(pl.LightningModule):
    def __init__(self, Train_classifier):
        super().__init__()
        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)
        if Train_classifier:
          for param in self.model.avgpool.parameters():
            param.requires_grad = False
          for param in self.model.features.parameters():
            param.requires_grad = False
          for param in self.model.classifier.parameters():
            param.requires_grad = True
        else: 
          for param in self.model.parameters():
            param.requires_grad = False
        # self.save_hyperparameters()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = F.cross_entropy(y_hat, y)
        self.log('train_loss', loss)
        return loss

    def training_epoch_end(self, outputs):
        losses = torch.as_tensor([o['loss'] for o in outputs])
        self.log('avg_train_loss', losses.mean(), prog_bar=True)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)

        loss = F.cross_entropy(y_hat, y)
        probs = F.softmax(y_hat, dim=1)
        classes_hat = torch.argmax(probs, dim=1)
        acc = torch.mean((classes_hat == y).type(torch.float32))

        output = {
            'val_loss': loss
            , 'val_acc': acc
        }
        self.log_dict(output, prog_bar=True)

        return output

    def validation_epoch_end(self, outputs):
        losses = []
        accs = []

        for o in outputs:
            losses.append(o['val_loss'])
            accs.append(o['val_acc'])

        self.log_dict({
            'avg_val_loss': torch.as_tensor(losses).mean()
            , 'avg_val_acc': torch.as_tensor(accs).mean()
        }, prog_bar=True)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=0.003)

    def train_dataloader(self):
        train_dataset = datasets.MNIST(
            'mnist'
            , train=True
            , download=False
            , transform=transforms.Compose([
                  transforms.Resize((227, 227))
                , transforms.Grayscale(3)
                , transforms.ToTensor()
                , transforms.Normalize((0.1307,), (0.3081,))
            ])
        )

        train_loader = torch.utils.data.DataLoader(
            train_dataset
            , batch_size=64
            , num_workers=2
            , shuffle=True
        )

        return train_loader

    def val_dataloader(self):
        val_dataset = datasets.MNIST(
            'mnist'
            , train=False
            , download=False
            , transform=transforms.Compose([
                  transforms.Resize((227, 227))
                , transforms.Grayscale(3)
                , transforms.ToTensor()
                , transforms.Normalize((0.1307,), (0.3081,))
            ])
        )

        val_loader = torch.utils.data.DataLoader(
            val_dataset
            , batch_size=64
            , num_workers=2
        )

        return val_loader